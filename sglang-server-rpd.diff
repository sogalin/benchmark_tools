diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 05bc8d73..48d2aebd 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -109,6 +109,7 @@ from sglang.srt.utils import (
     suppress_other_loggers,
 )
 from sglang.utils import TypeBasedDispatcher, get_exception_traceback
+from rpdTracerControl import rpdTracerControl
 
 logger = logging.getLogger(__name__)
 
@@ -2124,83 +2125,93 @@ class Scheduler:
         num_steps: Optional[int],
         activities: Optional[List[str]],
     ) -> None:
-        if self.torch_profiler_activities:
-            return ProfileReqOutput(
-                success=False,
-                message="Profiling is already in progress. Call /stop_profile first.",
-            )
-
-        if output_dir is None:
-            output_dir = os.getenv("SGLANG_TORCH_PROFILER_DIR", "/tmp")
-        if activities is None:
-            activities = ["CPU", "GPU"]
-
-        self.torch_profiler_output_dir = output_dir
-        self.torch_profiler_activities = activities
-        logger.info(
-            "Profiling starts. Traces will be saved to: %s",
-            self.torch_profiler_output_dir,
-        )
-
-        activity_map = {
-            "CPU": torch.profiler.ProfilerActivity.CPU,
-            "GPU": torch.profiler.ProfilerActivity.CUDA,
-        }
-        torchprof_activities = [
-            activity_map[a] for a in activities if a in activity_map
-        ]
-
-        if torchprof_activities:
-            self.torch_profiler = torch.profiler.profile(
-                activities=torchprof_activities,
-                with_stack=True,
-            )
-            self.torch_profiler.start()
-
-        if "MEM" in activities:
-            torch.cuda.memory._record_memory_history(max_entries=100000)
-
-        if num_steps:
-            self.profiler_target_forward_ct = self.forward_ct + num_steps
-            # The caller will be notified when reaching profiler_target_forward_ct
-        else:
-            self.profiler_target_forward_ct = None
-            return ProfileReqOutput(success=True, message="Succeeded")
+        if self.tp_rank == 0:
+            self.rpd = rpdTracerControl()
+            self.rpd.start()
+            logger.info("rpd is enable")
+        return ProfileReqOutput(success=True, message="Succeeded")
+#        if self.torch_profiler_activities:
+#            return ProfileReqOutput(
+#                success=False,
+#                message="Profiling is already in progress. Call /stop_profile first.",
+#            )
+#
+#        if output_dir is None:
+#            output_dir = os.getenv("SGLANG_TORCH_PROFILER_DIR", "/tmp")
+#        if activities is None:
+#            activities = ["CPU", "GPU"]
+#
+#        self.torch_profiler_output_dir = output_dir
+#        self.torch_profiler_activities = activities
+#        logger.info(
+#            "Profiling starts. Traces will be saved to: %s",
+#            self.torch_profiler_output_dir,
+#        )
+#
+#        activity_map = {
+#            "CPU": torch.profiler.ProfilerActivity.CPU,
+#            "GPU": torch.profiler.ProfilerActivity.CUDA,
+#        }
+#        torchprof_activities = [
+#            activity_map[a] for a in activities if a in activity_map
+#        ]
+#
+#        if torchprof_activities:
+#            self.torch_profiler = torch.profiler.profile(
+#                activities=torchprof_activities,
+#                with_stack=True,
+#            )
+#            self.torch_profiler.start()
+#
+#        if "MEM" in activities:
+#            torch.cuda.memory._record_memory_history(max_entries=100000)
+#
+#        if num_steps:
+#            self.profiler_target_forward_ct = self.forward_ct + num_steps
+#            # The caller will be notified when reaching profiler_target_forward_ct
+#        else:
+#            self.profiler_target_forward_ct = None
+#            return ProfileReqOutput(success=True, message="Succeeded")
 
     def stop_profile(self) -> None:
-        if self.torch_profiler_activities is None:
-            return
-
-        logger.info("Stop profiling...")
-        if self.torch_profiler is not None:
-            self.torch_profiler.stop()
-            self.torch_profiler.export_chrome_trace(
-                os.path.join(
-                    self.torch_profiler_output_dir,
-                    str(time.time()) + f"-TP-{self.tp_rank}" + ".trace.json.gz",
-                )
-            )
-
-        if "MEM" in self.torch_profiler_activities:
-            memory_profile_path = os.path.join(
-                self.torch_profiler_trace_dir,
-                str(time.time()) + f"-TP-{self.tp_rank}-memory" + ".pickle",
-            )
-            torch.cuda.memory._dump_snapshot(memory_profile_path)
-            torch.cuda.memory._record_memory_history(enabled=None)
-
-        logger.info(
-            "Profiling done. Traces are saved to: %s",
-            self.torch_profiler_output_dir,
-        )
-        self.torch_profiler = None
-        self.torch_profiler_output_dir = None
-        self.torch_profiler_activities = None
-
-        if self.profiler_target_forward_ct:
-            self.send_to_tokenizer.send_pyobj(
-                ProfileReqOutput(success=True, message="Succeeded.")
-            )
+        if self.tp_rank == 0:
+            self.rpd.stop()
+            self.rpd.flush()
+            logger.info("rpd is done")
+        return ProfileReqOutput(success=True, message="Succeeded")
+#        if self.torch_profiler_activities is None:
+#            return
+#
+#        logger.info("Stop profiling...")
+#        if self.torch_profiler is not None:
+#            self.torch_profiler.stop()
+#            self.torch_profiler.export_chrome_trace(
+#                os.path.join(
+#                    self.torch_profiler_output_dir,
+#                    str(time.time()) + f"-TP-{self.tp_rank}" + ".trace.json.gz",
+#                )
+#            )
+#
+#        if "MEM" in self.torch_profiler_activities:
+#            memory_profile_path = os.path.join(
+#                self.torch_profiler_trace_dir,
+#                str(time.time()) + f"-TP-{self.tp_rank}-memory" + ".pickle",
+#            )
+#            torch.cuda.memory._dump_snapshot(memory_profile_path)
+#            torch.cuda.memory._record_memory_history(enabled=None)
+#
+#        logger.info(
+#            "Profiling done. Traces are saved to: %s",
+#            self.torch_profiler_output_dir,
+#        )
+#        self.torch_profiler = None
+#        self.torch_profiler_output_dir = None
+#        self.torch_profiler_activities = None
+#
+#        if self.profiler_target_forward_ct:
+#            self.send_to_tokenizer.send_pyobj(
+#                ProfileReqOutput(success=True, message="Succeeded.")
+#            )
 
     def open_session(self, recv_req: OpenSessionReqInput):
         # handle error
